{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解压缩: Train Station Codes and Chinese Names.zip\n",
      "解压缩: Monthly Taxi Population.zip\n",
      "解压缩: Train Line Codes.xlsx\n",
      "解压缩: PremiumBusServices.zip\n",
      "解压缩: Number of MRT  LRT Station.zip\n",
      "解压缩: Rail Length.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip_all_in_directory(directory):\n",
    "    # 遍历指定目录中的所有文件\n",
    "    for filename in os.listdir(directory):\n",
    "        # 构造完整文件路径\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # 检查文件是否是ZIP文件\n",
    "        if zipfile.is_zipfile(file_path):\n",
    "            # 打开ZIP文件\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # 解压到指定目录\n",
    "                zip_ref.extractall(directory)\n",
    "                print(f\"解压缩: {filename}\")\n",
    "\n",
    "# 使用你的目标文件夹路径替换\n",
    "target_directory = \"/Users/ukrys/coding/IS5126/group_assignment/data/Static_ 2024_07/PUBLIC TRANSPORT\"\n",
    "unzip_all_in_directory(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# JSON数据的URL\n",
    "url = \"https://dmprod-datasets.s3.ap-southeast-1.amazonaws.com/traffic-flow/data/trafficflow.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBUaDmFwLXNvdXRoZWFzdC0xIkcwRQIhAKXUgS13vAIo2dMvNai3Ts0E0yXebSTiCytus8GaGWsyAiBA6nJXiJVAWyI9Np6Yd40TPNhneFk7j3Yb%2FhufuLirUSrCBQheEAQaDDM0MDY0NTM4MTMwNCIMhOjDZ7He1hgRWLbzKp8FYvkRezPls7chZBeU%2F7%2B1kcFlL0tzmufMCKhzgglqB1J74BmUe7UuAQnSo46iblBLTxHq8lYBjVbx%2BfxzyHS5mxyiei2IM0lqs%2FefxQwKy%2BiTUa4ivRIMxGRs6oRIeMkzYbJA5o%2FufXIwx79SZy2FlydKOjzY7d%2BNSHZRHa%2B60QVuek3Ru0z54xJM5XTimp1jWl2KBj%2FS87BBNEZna3QDeQsSeTRadOJ7ym47KmMxIFHavsS8CEaYmdMrubaDtSEEZKJA7zFsOQNtiKjQF3e5e5D%2B2Zz1ANMFKaffUHkvvyWki4G6ryYpXLLMcC3dhFfXgP2w9HCeVwFa%2F2XXixIjl38Ii%2F7onJa5eAmE1kxMFFerNcYAy8kQdQfbEji6VMQmY4NoXpeFlIaEze15pYUG%2BeyCTn0hXUg2p%2FHh0tuwseTeBqadIemYPvaIDgph61pybJ5AvtnHbV56DIASewG1D8wC4aUtN2SqBEWfomDAyq9oK6o8Fyx2wHcSXE%2FznoT4c2FUGauyaaEs6L3hTqMRWgx8H%2B5YbAXbutGsm%2FeFCoafncBw5l5z%2BvPidM2QNGUEDjofHSXRrpZ0DB3%2BibxFak%2F2AQtavAdg6ETmj%2FHqc4E7tXcktakJGwtET5OgVbhRwF84sG0%2B3BPwvEbNqGPZJTmGir7CEj1G39bmWVZAxURf%2BdRPhAqCo8TZqsPVbQJQPVXDSDfd6HiruZ%2BqjKX%2FKCPT4V1j0hcaLz%2FrUKmrkUOeUzP15PYPwOAqn5%2BiChT219H3YHqgBiOCx8qdyeZGY1zBMU88YbvFxhtym0dBzy69gYg7GLiL%2B5h77s9YuZmt2tbSu%2BDGiCbI1GFGPxgDKCVnzbJ7%2FKbZhXFoPx510vSAoDf25h5nxxHkP8HCXNwwgKLltwY6sQEAgwTcU1kxmPVzsV1YUPIQZqf0grQVlpPlhpIF8GphLAq1T6AHBNnkESI3lhz7To%2BUdH3muyx8LbHKIdrYhE52LGzEC%2BAfy%2FfEfMQcYOkDQ%2FeHhowhk0iK0HWkLK5sKG7q0B4VQ1463b8psAji%2F6elPHRlXgJ0swbqS4g6b%2Fdja9hSdz17H%2BixSzVx%2FbKFtuO4K1AewTifK4XykTDBVcl8cSdPiOBfnWQt4l8b%2BvKSgmk%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240929T134753Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAU6UAMAS4O5KBSVHG%2F20240929%2Fap-southeast-1%2Fs3%2Faws4_request&X-Amz-Signature=f44a2ea7001e0a05f535da576dfea8e29c3a3608e8eababef667b08fc492cb1b\"\n",
    "\n",
    "# 发送HTTP GET请求\n",
    "response = requests.get(url)\n",
    "\n",
    "# 确保请求成功\n",
    "if response.status_code == 200:\n",
    "    # 将响应内容解析为JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # 将JSON数据保存到文件\n",
    "    with open('traffic_flow.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api-open.data.gov.sg/v2/real-time/api/psi\"\n",
    "# 空气污染的数据可查范围为2016-03-09 - now\n",
    "response = requests.get(url)\n",
    "\n",
    "# 确保请求成功\n",
    "if response.status_code == 200:\n",
    "    # 将响应内容解析为JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # 将JSON数据保存到文件\n",
    "    # with open('pollutant_2023.json', 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pollutant_20241003.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_date = datetime(2024, 2, 1)\n",
    "end_date = datetime(2024, 2, 29)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    url = f\"https://api-open.data.gov.sg/v2/real-time/api/psi?\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "#         if 'items' in data['data']:\n",
    "#             readings = data['data']['items'][0]['readings']\n",
    "#             readings['date'] = date_str \n",
    "#             data_list.append(readings)        \n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve data for {date_str}\")\n",
    "#         data_list.append({'date': date_str})\n",
    "    \n",
    "#     current_date += timedelta(days=1)\n",
    "#     time.sleep(0.1)\n",
    "    \n",
    "# res_df = pd.DataFrame(data_list)\n",
    "# res_df.to_csv('pollutant_202402.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:00+08:00'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f\"https://api-open.data.gov.sg/v2/real-time/api/psi?\"\n",
    "response = requests.get(url).json()\n",
    "response['data']['items'][0]['timestamp'].split(\"T\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，Markdown文件已保存至：/Users/ukrys/coding/IS5126/group_assignment/cowork.md\n"
     ]
    }
   ],
   "source": [
    "from nbconvert import MarkdownExporter\n",
    "from nbformat import read\n",
    "import os\n",
    " \n",
    "# 指定Jupyter Notebook文件的完整路径\n",
    "input_notebook_path = \"/Users/ukrys/coding/IS5126/group_assignment/cowork.ipynb\"\n",
    " \n",
    "# 指定输出Markdown文件的路径\n",
    "# 注意：这里假设Markdown文件保存在同一目录下，且文件名与原Notebook相同，只是扩展名不同\n",
    "output_markdown_path = os.path.splitext(input_notebook_path)[0] + \".md\"\n",
    " \n",
    "# 读取Notebook文件内容\n",
    "with open(input_notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = read(f, as_version=4)\n",
    " \n",
    "# 创建Markdown导出器实例\n",
    "exporter = MarkdownExporter()\n",
    " \n",
    "# 导出Notebook为Markdown格式\n",
    "markdown_content, resources = exporter.from_notebook_node(notebook)\n",
    " \n",
    "# 将Markdown内容写入文件\n",
    "with open(output_markdown_path, 'w', encoding='utf-8') as md_file:\n",
    "    md_file.write(markdown_content)\n",
    " \n",
    "print(f\"转换完成，Markdown文件已保存至：{output_markdown_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved for 2024-10-04 00:25:00\n",
      "Data saved for 2024-10-04 00:30:00\n",
      "Data saved for 2024-10-04 00:35:00\n",
      "Data saved for 2024-10-04 00:40:00\n",
      "Data saved for 2024-10-04 00:50:00\n",
      "Data saved for 2024-10-04 01:55:00\n",
      "Data saved for 2024-10-04 03:00:00\n",
      "Data saved for 2024-10-04 03:20:00\n",
      "Data saved for 2024-10-04 03:40:00\n",
      "Data saved for 2024-10-04 04:30:00\n",
      "Data saved for 2024-10-04 05:00:00\n",
      "Data saved for 2024-10-04 05:45:00\n",
      "Data saved for 2024-10-04 06:20:00\n",
      "Data saved for 2024-10-04 07:00:00\n",
      "Data saved for 2024-10-04 07:45:00\n",
      "Data saved for 2024-10-04 08:10:00\n",
      "Data saved for 2024-10-04 09:10:00\n",
      "Data saved for 2024-10-04 09:40:00\n",
      "Data saved for 2024-10-04 10:10:00\n",
      "Data saved for 2024-10-04 10:15:00\n",
      "Data saved for 2024-10-04 10:20:00\n",
      "Data saved for 2024-10-04 10:25:00\n",
      "Data saved for 2024-10-04 10:30:00\n",
      "Data saved for 2024-10-04 10:35:00\n",
      "Data saved for 2024-10-04 10:40:00\n",
      "Data saved for 2024-10-04 10:45:00\n",
      "Data saved for 2024-10-04 10:50:00\n",
      "Data saved for 2024-10-04 10:55:00\n",
      "Data saved for 2024-10-04 11:00:00\n",
      "Data saved for 2024-10-04 11:05:00\n",
      "Data saved for 2024-10-04 11:10:00\n",
      "Data saved for 2024-10-04 11:15:00\n",
      "Data saved for 2024-10-04 11:20:00\n",
      "Data saved for 2024-10-04 11:35:00\n",
      "Data saved for 2024-10-04 11:50:00\n",
      "Data saved for 2024-10-04 12:00:00\n",
      "Data saved for 2024-10-04 12:05:00\n",
      "Data saved for 2024-10-04 12:10:00\n",
      "Data saved for 2024-10-04 12:15:00\n",
      "Data saved for 2024-10-04 12:20:00\n",
      "Data saved for 2024-10-04 12:50:00\n",
      "Data saved for 2024-10-04 13:10:00\n",
      "Data saved for 2024-10-04 13:15:00\n",
      "Data saved for 2024-10-04 13:25:00\n",
      "Data saved for 2024-10-04 13:30:00\n",
      "Data saved for 2024-10-04 13:35:00\n",
      "Data saved for 2024-10-04 13:40:00\n",
      "Data saved for 2024-10-04 13:45:00\n",
      "Data saved for 2024-10-04 13:50:00\n",
      "Data saved for 2024-10-04 13:55:00\n",
      "Data saved for 2024-10-04 14:00:00\n",
      "Data saved for 2024-10-04 14:05:00\n",
      "Data saved for 2024-10-04 14:10:00\n",
      "Data saved for 2024-10-04 14:15:00\n",
      "Data saved for 2024-10-04 14:20:00\n",
      "Data saved for 2024-10-04 14:25:00\n",
      "Data saved for 2024-10-04 14:30:00\n",
      "Data saved for 2024-10-04 14:35:00\n",
      "Data saved for 2024-10-04 14:40:00\n",
      "Data saved for 2024-10-04 14:45:00\n",
      "Data saved for 2024-10-04 14:50:00\n",
      "Data saved for 2024-10-04 14:55:00\n",
      "Data saved for 2024-10-04 15:00:00\n",
      "Data saved for 2024-10-04 15:20:00\n",
      "Data saved for 2024-10-04 15:35:00\n",
      "Data saved for 2024-10-04 15:45:00\n",
      "Data saved for 2024-10-04 15:50:00\n",
      "Data saved for 2024-10-04 15:55:00\n",
      "Data saved for 2024-10-04 16:00:00\n",
      "Data saved for 2024-10-04 16:05:00\n",
      "Data saved for 2024-10-04 16:10:00\n",
      "Data saved for 2024-10-04 16:15:00\n",
      "Data saved for 2024-10-04 16:20:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def fetch_traffic_data():\n",
    "    url = \"https://datamall2.mytransport.sg/ltaodataservice/v3/TrafficSpeedBands\"\n",
    "    headers = {\n",
    "        'AccountKey': 'l9hZP/8WSwKzxAogbnA++w==',\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        last_updated_time = data.get('lastUpdatedTime', 'unknown_time')\n",
    "        os.makedirs('tf', exist_ok=True)\n",
    "        with open(f'tf/traffic_flow_{last_updated_time}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved for {last_updated_time}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "\n",
    "def fetch_pollutant_data():\n",
    "    url = \"https://api-open.data.gov.sg/v2/real-time/api/psi?\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        last_updated_time = data['data']['items'][0]['timestamp'].split(\"T\")[1]\n",
    "        os.makedirs('pt', exist_ok=True)\n",
    "        with open(f'pt/pollutant_{last_updated_time}.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        \n",
    "\n",
    "# Keep the script running\n",
    "while True:\n",
    "    fetch_pollutant_data()\n",
    "    fetch_traffic_data()\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
